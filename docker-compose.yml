version: '3.8'

services:
  zookeeper:
    image: wurstmeister/zookeeper:latest
    ports:
      - "2181:2181"

  kafka:
    image: wurstmeister/kafka:latest
    ports:
      - "9092:9092"
    environment:
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 10s
      timeout: 5s
      retries: 5

  # jobmanager:
  #   hostname: jobmanager
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.flink-python
  #   image: custom-flink-python:1.20
  #   ports:
  #     - "8081:8081"  # Flink UI
  #     - "6123:6123"
  #   environment:
  #     - JOB_MANAGER_RPC_ADDRESS=jobmanager
  #     - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=2
  #     - PATH=$PATH:/home/bash1989/anaconda3/envs/ml/bin
  #     - PYTHON_EXECUTABLE=/usr/bin/python3.10
  #   command: ["jobmanager"]
  #   volumes:
  #     - ./conf:/opt/flink/conf
  #     - ./ml-environment:/home/bash1989/anaconda3/envs/ml

  # taskmanager:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.flink-python
  #   image: custom-flink-python:1.20
  #   environment:
  #     - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=2
  #     - JOB_MANAGER_RPC_ADDRESS=jobmanager
  #     - PATH=$PATH:/home/bash1989/anaconda3/envs/ml/bin
  #     - PYTHON_EXECUTABLE=/usr/bin/python3.10
  #   depends_on:
  #     - jobmanager
  #   command: ["taskmanager"]
  #   volumes:
  #     - ./conf:/opt/flink/conf
  #     - ./ml-environment:/home/bash1989/anaconda3/envs/ml

  influxdb:
    image: influxdb:2.0
    ports:
      - "8086:8086"
    volumes:
      - influxdb-storage:/var/lib/influxdb2
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8086/health"]
      interval: 60s
      timeout: 20s
      retries: 5
    restart: always

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    depends_on:
      - influxdb
    volumes:
      - grafana-storage:/var/lib/grafana

  data_fetcher:
    restart: always
    build:
      context: .
      dockerfile: Dockerfile.fetcher  # Specify a new Dockerfile for data_fetcher.py
    container_name: data_fetcher
    command: ["python", "/app/data_fetcher.py"]  # Run data_fetcher.py as the main process
    volumes:
      - ./data_fetcher.py:/app/data_fetcher.py
      - ./config.yaml:/app/config.yaml
      - ./requirements.txt:/app/requirements.txt
    depends_on:
      kafka: # Ensure Kafka is running before starting this service
        condition: service_healthy
    networks:
      - default

  data_processor:
    restart: always
    build:
      context: .
      dockerfile: Dockerfile.processor  # Specify a new Dockerfile for data_processor.py
    container_name: data_processor
    command: ["python", "/app/data_processor.py"]  # Run data_processor.py as the main process
    volumes:
      - ./data_processor.py:/app/data_processor.py
      - ./config.yaml:/app/config.yaml  # Mount config.yaml
      - ./requirements.txt:/app/requirements.txt
    depends_on:
      kafka: # Ensure Kafka is running before starting this service
        condition: service_healthy
      influxdb: # Ensure Kafka is running before starting this service
        condition: service_healthy
    networks:
      - default


  # flink_data_writer:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.flink_writer  # Specify a new Dockerfile for flink_data_writer.py
  #   container_name: flink_data_writer
  #   command: ["python", "flink_data_writer.py"]  # Run flink_data_writer.py as the main process
  #   volumes:
  #     - ./flink_data_writer.py:/app/flink_data_writer.py  # Mount flink_data_writer.py
  #     - ./config.yaml:/app/config.yaml  # Mount config.yaml
  #   depends_on:
  #     - kafka  # Ensure Kafka is running before starting this service
  #     - influxdb  # Ensure InfluxDB is available
  #   networks:
  #     - default



volumes:
  influxdb-storage:
  grafana-storage:
